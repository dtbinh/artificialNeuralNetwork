\chapter{Neural Network}

\section{MultiLayerPerceptron} \label{refPositionNeuronTo}
The artificial neural network is built by the MultiLayerPerceptron-class. The class arranges a specified number of neurons into several layers. At least the input layer, the output layer, and one hidden layer will be built.

\subsection{Constructors}
The following constructors are available:
\begin{itemize}
	\item \texttt{MultiLayerPerceptron(int nrInputNeurons, int[] nrHiddenNeurons, int nrOutputNeurons)}\\
	\item \texttt{MultiLayerPerceptron(int nrInputNeurons, int nrHiddenNeuronsPerLayer, int numberHiddenLayers, int nrOutputNeurons)}\\
	\item \texttt{MultiLayerPerceptron(int nrInputNeurons, int[] nrHiddenNeurons, int nrOutputNeurons, String inputTopology, String hiddenTopology, String outputTopology)}\\
\end{itemize}
The two more simple constructors call the third constructor and set the missing values to default values.\\
The fist constructor ust needs the number of neurons for the input, output, and each hidden layer. The topologies for the interconnection between the layers are set to ``each''. (Topologies are explained further down.)
\\ The second constructor can be use if all hidden layers shall have the same number of neurons. While the other two constructors set the number of hidden layers by the size of the \texttt{nrHiddenNeurons}-integer-array, this constructor has a dedicated variable for the number of hidden layers. as well as the constructor befor this construcor sets  the topologies to ``each''.\\
The last constructor basically consists of two parts. The first part prepares the neurons, the second part builts the connections.

\subsection{Topologies}
As can be seen from the construcor there are three different areas for topologies. The connections between the input layer and the first hidden layer, the connections between the hidden layers, and the connections between the last hidden layer and the output layer. For each of these three areas there are the following three types of interconnection possible:
\begin{itemize}
	\item one: Each neuron of the preceding layer will be connected to just one neuron of the following layer. In case there are more neurons in the following layer than in the preceding layer, neurons in the middle of the smaller layer will be connected to two ore more neurons. In case there are less neurons in the following layer, neurons in the middle from the following layer will be connected to two or more neurons from the preceding layer.\\
	\item two groups: Both layers are split in the middle. Each neuron of one group in the preceding layer will be connected to each neuron of the same group in the following layer.
	\item each: Each neuron of the preceding layer will be connected to each neuron of the preceeding layer.\\
\end{itemize}

\subsection{Building the neurons}
While the calculation itself varies depending on the chosen topology, the steps itself are similar. In a special variable the needed inputs for the each neuron of the following layer are stored. In case of the topology ``one'' this value is 1, if the amount of neurons in both layers are equal. If there are more neurons in the preceding layer a range around the middle of the following layer is calculated where one more input is needed. Using the topology ``two groups'' the number of inputs is neurons of the preceding layer divided by two. In case of topology ``each'' the number of inputs is equal to the amount of the neurons of the preceding layer. In case of the input layer each input neuron is implemented with one input.\\
The threshold of each neuron is initialised with 0.

\subsection{Building the connections}
Each layer area is stored in a dedicated array. Since there might be more than 1 hidden layer the hidden layers are stored in an array-list.\\
Depending on the chosen topology the connections are built. For each neuron at least one following neuron, depending on the topology, is calulated.\\
In case of the topology ``one'' 
% und cross und zigzag
there is an differentation if the current layer is larger than the next one. This can be seen as choosing a starting point. In case the current layer is smaller the calculation uses the neurons of this layer, noted as n in sourcecode, as starting point. Is the current layer is larger, than the neurons of the following layer, notes as m, are used.\\
The weights of all connections is initialised with 1.

%Vorüberlegungen in case one, Inputlayer ca. l.100
%\begin{lstlisting}
%/***
%* 		O
%* 		I -> rangeStart
%* 		I
%* 		I -> rangeStart + range
%* 		O -> n.length-1
%* 
%* 		-> m.length - range = Anzahl Neuronen ohne extra Input
%* 		-> (m.length - range) / 2 = Anzahl Neuronen ohne extra Input unter-/überhalb range
%* 		=> rangeStart  = (m.length - ((m.length-range) / 2)) - range
%*/
%\end{lstlisting}

\subsection{Executing the network}
To execute the built network the method \texttt{float[] run(float[] inputVector)} has to be called. The given input vector is passed to the input neurons by calling their \texttt{setInput}-method. In three consecutive loops, on for each layer area, the connections of each layer are executed one after another by calling their \texttt{run()}-methods. The last step is to get the result vector by calling the \texttt{getOutput}-method of the output neurons.

\subsection{Training of the network}

%!!! Vorüberlegungen sichern!!! Richtiger Source Code geändert!\\
% 1. Teil
\begin{lstlisting}
/****************************************************************************************
	* TODO: Klaeren ob inkl. threshold
	*	Kommentare in TechDoc
	*	
	* ---------------------------------------------------------------------------------------
	* Beispiel:	2 Input, 4 Hidden, 2 Hiddenlayer, 2 Output
	*		Connection each, Oupuzt connection 2 Groups
	*              ----------------------------------------------------------------
	*		inputConnection[] | hiddenConnection[][] | outputConnection[]
	*	0 :	0 >			0 >			0
	*					1
	*					2
	*					3
	*		1 >			0
	*					1
	*					2
	*					3
	*		2 >			0
	*					1 >			0
	*					2
	*					3
	*		3 >			0
	*					1
	*					2
	*					3
	*	1 :	0 >			0
	*					1
	*					2 >			1
	*					3
	*		1 >			0
	*					1
	*					2
	*					3
	*		2 >			0
	*					1
	*					2
	*					3 >			1
	*		3 >			0
	*					1
	*					2
	*					3
	*              ----------------------------------------------------------------
	* ---------------------------------------------------------------------------------------
	*
	* Backpropagation allgemein:
	* deltaGewichtsvektor_u = Lernfaktor (Summe über Folgeneuronen((Summe über Ausgabeneuronen(
	* (Erwartete Ausgabe - Ausgabe) dAusgabe_Ausgabeneuron nach dNetzeingabe_Folgeneuron))*Gewicht_Neuron-FolgeNeuron)
	* ) dAusgabe_u nach dNetzeingabe_u * Eingabevektor_u (S.71)
	*
	* Für Identiaet als Ausgabefunktion und logistischer Aktivierungsfunktion:
	* deltaGewichtsvektor_u = Lernfaktor (Summe über Folgeneuronen((Summe über Ausgabeneuronen(
	* (Erwartete Ausgabe_Ausgabeneuron - Ausgabe_Ausgabeneuron)dAusgabe_Ausgabeneuron nach dNetzeingabe_Folgeneuron))*Gewicht_FolgeNeuron-u)
	* ) Ausgabe_u (1 - Ausgabe_u) * Eingabevektor_u
	
	* -> SCHICHTENWEISE
	-> Summe über Ausgabeneuronen:
	* -> Summe über Folgeneuronen: Pfad im Netz muss mathematisch nachgebildet werden
	* -> Eingabevektor muss angepasst werden, enthaelt Bitmuster
	* -> bei logistischer Funktion:
	*	dAusgabe_Ausgabeneuron nach dNetzeingabe_Folgeneuron = Ausgabe_Ausgabeneuron (1 - Ausgabe_Ausgabeneuron)
	*
	* Einzelner Gradientenabstieg:
	* delta Gewichtsvektor_u = Lernfaktor (Erwartete Ausgabe - Ausgabe) Ausgabe (1 - Ausgabe) * Eingabevektor_u
	* -> pro inputneuron:
	* delta Gewicht_u = Lernfaktor (Erwartete Ausgabe - Ausgabe) Ausgabe (1 - Ausgabe) * Eingabe_u
	*
	* Um deltaGewicht für jede verbindung zu berechnen erst mit for-loop über outputconnection.length
	* dann for (int i = hiddenLayer-2; i>=0; i++) -> loop über hiddenLayer[i].length
	* dann for-loop über inputConnection.length
	*
	* Um Folgeneuronen zu ermitteln Position in Array benötigt, dann loops über die folgende
	* Netzstruktur (nicht einfach über die ConnectionArrays)
	****************************************************************************************/
private	void backpropagation(float[] resultOut, float[] wantedOut){
		// TODO: Kommentare wofür Variablen, Richtige Reihenfolge
		int connectionsOfNeuron;
		int neuronsInLayer;
		int neuronsInNextLayer;
		int offset;
		float trainingCoefficient = 0.2f;
		float weightDelta = 0;
		float connWeights[];
		Connection connections[];
		float outputOfNeuron;
		float inputVectorOfNeuron;
		
		// Um deltaGewicht für jede verbindung zu berechnen loop über alle (Verbindungs-)Layer...
		for (int layer = hiddenLayers; layer >= 0; layer--){
			// Bestimmen der Position des Neurons im Netz und der folgenden
			// Struktur für die Summe über die Folgeneuronen,
			// d.h. für die Grenze der for-Schleife. Daher nur int neuronsInLayer benötigt
			// -> mit layer ermitteln in welchem Layer: output, hidden o input
			// -> mit neuron an welcher Stelle im Layer
			// Grenze für Summe über Folgeneuronen
			
			// DEBUG
			System.out.println("int layer = " + hiddenLayers + "; layer >= 0; layer: " + layer + "{");
			
			// Outputconnection layer
			if (layer == hiddenLayers){
				neuronsInLayer = hiddenNeuron[layer-1].length;
				neuronsInNextLayer = 1;
				
				// Es gibt nur Verbindung(en) vom letztem Hidden Layer zu OutputNeuron(en)
				
				// TODO: Abbruchbedingung, ist noch falsch, benötigt allgemein gültigen Ausdruck
				// Go to next output neuron if all inputs are connected
				// Funktioniert nur wenn neuron > 0
				//while ((neuron % (hiddenNeuron[hiddenLayers-1].length/outputNeuron.length)) != 0){}
				
				// Im Moment nur eine Verbindung von hidden Neuron zu outputLayer
				connectionsOfNeuron = 1;
				
			/***************************************************************
			// Connections between last hidden and output layer
			int inp = 0;
			int outNeur = -1;
				
			// From each hidden neuron...
			for (int hidNeur = 0; hidNeur < hiddenNeuron[hiddenLayers-1].length; hidNeur++, inp++){
					
				// Go to next output neuron if all inputs are connected
				if ((hidNeur % (hiddenNeuron[hiddenLayers-1].length/outputNeuron.length)) == 0){
					outNeur++;
					inp = 0;
				}
					
				// ...to the "nearest" output neuron (if several are available)
				outputConnection[hidNeur] = new Connection(hiddenNeuron[hiddenLayers-1][hidNeur],
						outputNeuron[outNeur], inp, outputConnWeights[hidNeur]);
			}
			***************************************************************/
			
				// TODO: Auch hier allgemeine Formulierungen
				connWeights = new float [outputConnWeights.length];
				connWeights = outputConnWeights;
				
				connections = new Connection [outputConnection.length];
				connections = outputConnection;
			}
			
			// InputConnection layer
			else if (layer == 0){
				neuronsInLayer = inputNeuron.length;
				neuronsInNextLayer = hiddenNeuron[0].length;
				
				// TODO: Allgemein gültiger Ausdruck
				// Im Moment nur von jeden inputNeuron zu jedem hiddenNeuron
				connectionsOfNeuron = hiddenNeuron[0].length;
				
			/***************************************************************
			// Connections between input layer and 1st hidden layer
			int positionInLayer = 0;
			
			// From each input neuron...
			for (int inp = 0; inp < inputNeuron.length; inp++){
			
				// ...to each neuron of the 1st hidden layer
				for (int conn = 0; conn < hiddenNeuron[0].length; conn++){
					
					inputConnection[positionInLayer] = new Connection(inputNeuron[inp],
							hiddenNeuron[0][conn], inp, inputConnWeights[positionInLayer]);
					
					positionInLayer++;
				}
			}
			***************************************************************/
			
				connWeights = new float [inputConnWeights.length];
				connWeights = inputConnWeights;

				connections = new Connection [inputConnection.length];
				connections = inputConnection;
						
			/***************************************************************
			// Connection weights
			inputConnWeights = new float[inputNeurons * hiddenNeuronsPerLayer];
			
			hiddenConnWeights = new float[hiddenLayers-1][hiddenNeuronsPerLayer * hiddenNeuronsPerLayer];

			outputConnWeights = new float[hiddenNeuronsPerLayer];
			
			Struktur siehe Connections weiter oben
			***************************************************************/
			}
			
			// Hidden Layer
			else {
				neuronsInLayer = hiddenNeuron[layer-1].length;
				neuronsInNextLayer = hiddenNeuron[layer].length;
				
				// TODO: Allgemein gültiger Ausdruck
				// Im Moment nur von jeden hiddenNeuron zu jedem hiddenNeuron im Folgelayer
				connectionsOfNeuron = hiddenNeuron[layer].length;
				
				connWeights = new float [hiddenConnWeights[layer-1].length];
				connWeights = hiddenConnWeights[layer-1];

				connections = new Connection [hiddenConnection[layer-1].length];
				connections = hiddenConnection[layer-1];
			}
			//DEBUG
			System.out.println("\tconnWeights.lenght: " + connWeights.length);
			
			// ..., über alle Neuronen in dem Layer...
			for (int neuron = 0; neuron < neuronsInLayer; neuron++){
				//DEBUG
				System.out.println("\tfor (int neuron = 0; neuron < "+neuronsInLayer+"; neuron: "+neuron+"{");
				
				// Position of 1st connection of this neuron (Is the same
				// for all neurons in this layer)
				offset = neuron * connectionsOfNeuron;
			
				// ...und nu über alle Verbindungen des Neurons
				for (int conn = 0; conn < connectionsOfNeuron; conn++){
					//DEBUG
					System.out.println("\t\tfor (int conn = 0; conn < "+connectionsOfNeuron+"; conn" +conn+ "{");
					
					/*** Notwendig?
					// pro Inputneuron:
					// Notwendig? wie sonst inputVector[inp] verwenden?
					for (int inp = 0; inp < inputNeuron.length; inp++){
					}// for() pro Inputneuron
					***/
					
					// Beginn der Berechnung
					// deltaGewichtsvektor_u = Lernfaktor * -> hinter die Summen verschoben
					// weightDelta = trainingCoefficient * 
					
					// DEBUG
					System.out.println("\t\tneuronsInNextLayer: " + neuronsInNextLayer);
					
					// Summe über Folgeneuronen
					for (int succ = 0; succ < neuronsInNextLayer; succ++){
						
						// Summe über Ausgabeneuronen
						for (int out = 0; out < outputNeuron.length; out++){
							weightDelta += (
							// Erwartete Ausgabe_Ausgabeneuron - Ausgabe_Ausgabeneuron
							(wantedOut[out] - resultOut[out]) *
							//dAusgabe nach dNetzeingabe
							resultOut[out] * (1- resultOut[out]));
						}// for() Summe über Ausgabeneuronen
						
						// Bestimmen der Position im Netz um Gewicht_Neuron-FolgeNeuron zu laden
						// layer und neuron: Position von Neuron im Netz
						// connectionsOfNeuron: Anzahl der Folgeneuronen von Neuron
						// succ: x-tes FolgeNeuron von Neuron
						
						// * Gewicht_Neuron-FolgeNeuron
						
						//DEBUG
						System.out.println("\t\t\tconnWeights["+ (offset+succ) + "]");
						
						weightDelta *= connWeights[offset + succ];
					}// for() Summe über Folgeneuronen
					
					// * Ausgabe_u (1 - Ausgabe_u) * Eingabevektor_u
					// Ausgabe_u	= getOutput von aktuellem Neuron -> connection[neuron + conn].getNeuronTo.getOutput
					// Eingabevektor_u	= inputs[] von aktuellem Neuron -> getInputVector() in Neuron implementieren
					// Eingabevektor_u = netInput, da Verbindungsgewichte schon in inputs integriert
					outputOfNeuron = connection[neuron + conn].getNeuronTo.getOutput;
					inputVectorOfNeuron = connection[neuron + conn].getNeuronTo.getNetInput;
					
					weightDelta *= (outputOfNeuron * (1 - outputOfNeuron) * inputVectorOfNeuron;
					
					// deltaGewichtsvektor_u = Lernfaktor * -> hinter die Summen verschoben
					weightDelta *= trainingCoefficient;
					
					// Add calculated weight difference to connection weight
					connections[neuron + conn].addWeightDelta(weightDelta);
				}// for() über alle Verbindungen
			}// for() über alle Neuronen
		}// for() über alle (Verbindungs-)Layer
	}// backpropagation()
\end{lstlisting}
% 2.Teil
\begin{lstlisting}
	/****************************************************************************************
	* TODO: Aufraeumen
	*	
	* ---------------------------------------------------------------------------------------
	* Beispiel:	2 Input, 4 Hidden, 2 Hiddenlayer, 2 Output
	*		Connection each, Oupuzt connection 2 Groups
	*              ----------------------------------------------------------------
	*		inputConnection[] | hiddenConnection[][] | outputConnection[]
	*	0 :	0 >			0 >			0
	*					1
	*					2
	*					3
	*		1 >			0
	*					1
	*					2
	*					3
	*		2 >			0
	*					1 >			0
	*					2
	*					3
	*		3 >			0
	*					1
	*					2
	*					3
	*	1 :	0 >			0
	*					1
	*					2 >			1
	*					3
	*		1 >			0
	*					1
	*					2
	*					3
	*		2 >			0
	*					1
	*					2
	*					3 >			1
	*		3 >			0
	*					1
	*					2
	*					3
	*              ----------------------------------------------------------------
	* ---------------------------------------------------------------------------------------
	*
	* Backpropagation allgemein:
	* deltaGewichtsvektor_u = Lernfaktor (Summe über Folgeneuronen((Summe über Ausgabeneuronen(
	* (Erwartete Ausgabe - Ausgabe) dAusgabe_Ausgabeneuron nach dNetzeingabe_Folgeneuron))*Gewicht_Neuron-FolgeNeuron)
	* ) dAusgabe_u nach dNetzeingabe_u * Eingabevektor_u (S.71)
	*
	* Für Identiaet als Ausgabefunktion und logistischer Aktivierungsfunktion:
	* deltaGewichtsvektor_u = Lernfaktor (Summe über Folgeneuronen((Summe über Ausgabeneuronen(
	* (Erwartete Ausgabe_Ausgabeneuron - Ausgabe_Ausgabeneuron)dAusgabe_Ausgabeneuron nach dNetzeingabe_Folgeneuron))*Gewicht_FolgeNeuron-u)
	* ) Ausgabe_u (1 - Ausgabe_u) * Eingabevektor_u
	
	* -> SCHICHTENWEISE
	-> Summe über Ausgabeneuronen:
	* -> Summe über Folgeneuronen: Pfad im Netz muss mathematisch nachgebildet werden
	* -> Eingabevektor muss angepasst werden, enthaelt Bitmuster
	* -> bei logistischer Funktion:
	*	dAusgabe_Ausgabeneuron nach dNetzeingabe_Folgeneuron = Ausgabe_Ausgabeneuron (1 - Ausgabe_Ausgabeneuron)
	*
	* Einzelner Gradientenabstieg:
	* delta Gewichtsvektor_u = Lernfaktor (Erwartete Ausgabe - Ausgabe) Ausgabe (1 - Ausgabe) * Eingabevektor_u
	* -> pro inputneuron:
	* delta Gewicht_u = Lernfaktor (Erwartete Ausgabe - Ausgabe) Ausgabe (1 - Ausgabe) * Eingabe_u
	*
	* Um deltaGewicht für jede verbindung zu berechnen erst mit for-loop über outputconnection.length
	* dann for (int i = hiddenLayer-2; i>=0; i++) -> loop über hiddenLayer[i].length
	* dann for-loop über inputConnection.length
	*
	* Um Folgeneuronen zu ermitteln Position in Array benötigt, dann loops über die folgende
	* Netzstruktur (nicht einfach über die ConnectionArrays)
	*
	* Umwandlung des Schwellenwertes in ein Gewicht: Der Schwellenwert wird auf 0 festgelegt, als Ausgleich wird ein zusaetzlicher (imaginaerer)
	* Eingang (x_0) eingefuehrt, der den festen Wert 1 hat und mit dem negierten Schwellenwert gewichtet wird.(S.32)
	****************************************************************************************/
private	void backpropagation(float[] resultOut, float[] wantedOut){
		// TODO: wofür Variablen, Richtige Reihenfolge
		float trainingCoefficient = 2;
		float weightDelta = 0;
		float succWeights[] = new float[1];
		Connection connections[] = new Connection[1];
		int succNeurons = 0;
		int numberOfThresholds;
		float outputOfNeuron;
		float inputOfNeuron;
		int position;
		
		// Um deltaGewicht für jede verbindung zu berechnen erst loop über alle (Verbindungs-)Layer...
		int connectionLayer = hiddenLayers - 1;
		
		for (int layer = connectionLayer; layer >= 0; layer--){
			
			// Outputconnection layer
			if (layer == connectionLayer){
				connections = new Connection [outputConnections.length];
				connections = outputConnections;
				
				numberOfThresholds = outputNeurons.length;
			}
			
			// InputConnection layer
			else if (layer == 0){
				if (connectionLayer > 1){
					succWeights = new float [hiddenConnWeights[0].length];
					succWeights = hiddenConnWeights[0];
					
					// Number of inputs of neuron in 1st hidden layer = number of
					// successive neurons
					succNeurons = hiddenConnWeights[0].length/hiddenNeurons[0].length;
				}
				else {
					succWeights = new float [outputConnWeights.length];
					succWeights = outputConnWeights;
					
					succNeurons = outputConnWeights.length/outputNeurons.length;
				}

				connections = new Connection [inputConnections.length];
				connections = inputConnections;
				
				numberOfThresholds = inputNeurons.length;
			}
			
			// Hidden Layer
			else {
				if (layer == connectionLayer - 1){
					succWeights = new float [outputConnWeights.length];
					succWeights = outputConnWeights;
					
					// Nur eine Verbingung pro Neuron zum outputLayer
					succNeurons = 1;
				}

				else {
					succWeights = new float [hiddenConnWeights[layer + 1].length];
					succWeights = hiddenConnWeights[layer + 1];
				
					// To each neuron of the following layer
					succNeurons = hiddenNeurons[layer + 1].length;
				}
				
				connections = new Connection [hiddenConnections[layer].length];
				connections = hiddenConnections[layer];
				
				numberOfThresholds = hiddenNeurons[layer].length;
			}
			
			// ...und nu über alle Verbindungen
			for (int conn = 0; conn < (connections.length); conn++){
				
				// Gradientenabstieg für outputLayer
				if (layer == connectionLayer){
					// delta Gewichtsvektor_u = Lernfaktor (Erwartete Ausgabe - Ausgabe) Ausgabe (1 - Ausgabe) * Eingabevektor_u
					// -> delta Gewichtsvektor_u = Lernfaktor Ausgabe (1 - Ausgabe) * Eingabevektor_u auch in backpropagation (hinter for-loops)
					// -> nur (Erwartete Ausgabe - Ausgabe) benötigt
					position = connections[conn].getPositionNeuronTo();
					
					weightDelta = wantedOut[position] - resultOut[position];
				}
				
				// Da real backpropagation
				else {
					// Beginn der Berechnung
					// deltaGewichtsvektor_u = Lernfaktor * -> hinter die Summen verschoben
					// weightDelta = trainingCoefficient * 
				  
					// Summe über Folgeneuronen
					for (int succ = 0; succ < succNeurons; succ++){
						int sumOutputNeurons = 0;
						
						// Summe über Ausgabeneuronen
						for (int out = 0; out < outputNeurons.length; out++){
							sumOutputNeurons += (
							// Erwartete Ausgabe_Ausgabeneuron - Ausgabe_Ausgabeneuron
							(wantedOut[out] - resultOut[out]) *
							//dAusgabe nach dNetzeingabe
							resultOut[out] * (1- resultOut[out]));
						}// for() Summe über Ausgabeneuronen
				
						weightDelta += sumOutputNeurons;
						
						// * Gewicht_Neuron-FolgeNeuron
						position = connections[conn].getPositionNeuronTo();
						
						weightDelta *= succWeights[position];
					}// for() Summe über Folgeneuronen
				}
				
				// * Ausgabe_u (1 - Ausgabe_u) * Eingabevektor_u
				// Ausgabe_u	= getOutput von aktuellem Neuron -> connection[neuron + conn].getNeuronTo.getOutput
				// Eingabevektor_u	= inputs[] von aktuellem Neuron -> getInputVector() in Neuron implementieren
				outputOfNeuron = connections[conn].getNeuronTo().getOutput();
				inputOfNeuron = connections[conn].getNeuronFrom().getOutput();
					
				weightDelta *= (outputOfNeuron * (1 - outputOfNeuron) * inputOfNeuron);
					
				// deltaGewichtsvektor_u = Lernfaktor * -> hinter die Summen verschoben
				weightDelta *= trainingCoefficient;
					
				// Add calculated weight difference to connection weight
				connections[conn].addWeightDelta(weightDelta);
			}// for() über alle Verbindungen
			
			// for über die Thresholds
			for (int thresh = 0; thresh < numberOfThresholds; thresh++){
				// TODO: Der Schwellenwert wird auf 0 festgelegt??? noch notwendig?
				// , als Ausgleich wird ein zusaetzlicher (imaginaerer)
				// Eingang (x_0) eingeführt, der den festen Wert 1 hat und mit dem negierten Schwellenwert gewichtet wird.(S.32)
				// Position in Layer = threshold * Anzahl der Verbindungen pro Neuron -> Anzahl der Verbinungen in Layer / Anzahl der Neuronen (Thresholds)
				int positionInLayer = thresh * (connections.length / numberOfThresholds);
				float oldThresh = -1 * connections[positionInLayer].getNeuronTo().getThreshold();
				
				// Gradientenabstieg für outputLayer
				if (layer == connectionLayer){
					// delta Gewichtsvektor_u = Lernfaktor (Erwartete Ausgabe - Ausgabe) Ausgabe (1 - Ausgabe) * Eingabevektor_u
					// -> delta Gewichtsvektor_u = Lernfaktor Ausgabe (1 - Ausgabe) * Eingabevektor_u auch in backpropagation (hinter for-loops)
					// -> nur (Erwartete Ausgabe - Ausgabe) benötigt
					// Erwartete Ausgabe - Ausgabe
					// Note: Anzahl thresholds entspricht Anzahl, und somit Position, von OutputNeuron
					weightDelta = wantedOut[thresh] - resultOut[thresh];
				}
				
				// Da real backpropagation
				else {
					// Beginn der Berechnung
					// deltaGewichtsvektor_u = Lernfaktor * -> hinter die Summen verschoben
					// weightDelta = trainingCoefficient * 
				  
					// Summe über Folgeneuronen
					for (int succ = 0; succ < succNeurons; succ++){
						int sumOutputNeurons = 0;
						
						// Summe über Ausgabeneuronen
						for (int out = 0; out < outputNeurons.length; out++){
							sumOutputNeurons += (
							// Erwartete Ausgabe_Ausgabeneuron - Ausgabe_Ausgabeneuron
							(wantedOut[out] - resultOut[out]) *
							//dAusgabe nach dNetzeingabe
							resultOut[out] * (1- resultOut[out]));
						}// for() Summe über Ausgabeneuronen
				
						weightDelta += sumOutputNeurons;
						
						// * Gewicht_Neuron-FolgeNeuron
						weightDelta *= succWeights[succ];
					}// for() Summe über Folgeneuronen
				}
				
				// * Ausgabe_u (1 - Ausgabe_u) * Eingabevektor_u
				// Ausgabe_u	= getOutput von aktuellem Neuron -> connection[neuron + conn].getNeuronTo.getOutput
				// Eingabevektor_u	= inputs[] von aktuellem Neuron -> getInputVector() in Neuron implementieren				
				outputOfNeuron = connections[positionInLayer].getNeuronTo().getOutput();
				// inputVectorOfNeuron nicht benötigt, da nur = 1;
					
				weightDelta *= (outputOfNeuron * (1 - outputOfNeuron));
					
				// deltaGewichtsvektor_u = Lernfaktor * -> hinter die Summen verschoben
				weightDelta *= trainingCoefficient;
					
				// Add calculated weight difference to old threshold and set new threshold
				connections[positionInLayer].getNeuronTo().setThreshold(oldThresh + weightDelta);
			}// for() über thresholds
		}// for() über alle (Verbindungs-)Layer
	}// backpropagation()
\end{lstlisting}

%Training mit training coefficient is defined as 0.2.

\subsection{Finding the fastest network}